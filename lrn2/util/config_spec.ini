################################################################
# Specification for configuration files of the lrn2 framework. #
# Add new mix-in NN classes in the 'type' list of the          #
# TOPOLOGY-__many__ section, in order to use it with a         #
# config.ini file.                                             #
################################################################


[OUTPUT]
output_path = string(max=1000)
img_interval = integer(default=10)
dump_interval = integer(default=50)

[INPUT]
ngram_size = integer(default=10)

[TOPOLOGY]
[[__many__]]
type_net = option('Custom', 'RBMConvPCD','RBMConvCD','RBMConvGauss','RBMPCD','RBMCDGOH','RBMPCDGOH', 'RBMCDGauss','RBMPCDReLU', 'NNConvLinearBN', 'NNConvSigmoidBN', 'NNConvSigmoidAuto', 'NNConvReLUBN', 'NNDConvSigmoid', 'NNDConvReLU','NNLinearBN', 'NNSigmoidBN', 'NNReLUBN', 'NNSoftmaxBN', 'AutoEncoder', 'RNNLSTM', 'RNNPredict', 'RNNGated', 'MaxPooling', 'MaxPoolingProb', 'MaxPoolingOverlap', 'UpSampler', 'TransitionFunction', 'TransitionFunctionConv', 'Normalizer','ConvShaper', 'ConvDShaper', 'ConvToDense')
custom_type = string(max=50, default="None")
convolutional = boolean(default=False)
no_param_layer = boolean(default=False)

filter_shape = int_list(default=list(-1,-1)) # size of a filter in convolutional types
downsample_out = int_list(default=list(1,1))
n_hidden = integer(default=128)
n_out = integer(default=128)    # for recurrent nets

# desired out shape for de-convolution: # next units # next x # next y
out_shape = int_list(default=list(-1,-1,-1)) 

## Training ##
epochs = integer(default=500)
batch_size = integer(default=250)
learning_rate = float(default=0.01)
reduce_lr = boolean(default=False)
momentum = float(default=0.7)

n_cd = integer(default=1)
fantasy_particles = integer(default=None)
reset_pps_int = integer(default=-1)

## Regularizer ##
grad_clip = float(default=None)
wl1 = float(default=0.0)
wl2 = float(default=0.0)
max_norm = float(default=1e+200)

# Lee et al (2007) sparsity regularization
sparsity = float(default=0.5)
sparsity_strength = float(default=0.0)
narrow = float(default=0.0) # pushes hidden activations towards sparsity value

sparsity_sum = float(default=0.0) # pushes small hidden activations towards 0

# KL sparsity
sparsity_kl = float(default=0.5)
sparsity_kl_strength = float(default=0.1)

# Goh et al (2010) sparsity (mainly for RBMs)
mu = float(default=0.5) 
phi = float(default=0.0)

entropy_reg = float(default=0.0) # causes low entropy in hidden unit activations

non_negativity = float(default=0.0)
noise_level = float(default=0.0)

# Variance fixer (fixed variance of hidden units in stack prevents vanishing gradients)
trg_variance = float(default=4.)
variance_fix_strength = float(default=1000.)

dropout_h = float(default=0.0)
dropout_v = float(default=0.0)

bias_h = float(default=0.0)
ext_bias = option('v', 'h', 'both', default='v')

activation_crop = float(default=1.0)

# Specialized

# downscale factors for pooling layers
downsample = int_list(default=list(1,1))

# downscale factors for upsampling layers
upsample = int_list(default=list(1,1)) 

# strided convolution
stride = int_list(default=list(1,1)) 

smooth = float(default=0.0)
filt_size_self_sim = int_list(default=list(8,8)) # self-similarity tiling
sb_max = float(default = 1e-2)
compile_functions = boolean(default=True)

[STACK]
img_interval = integer(default=10)
dump_interval = integer(default=50)
learning_rate = float(default=0.000005)
grad_clip = float(default=None)
momentum = float(default=0.85)

noise_level = float(default=0.0)
epochs = integer(default=20)

reduce_lr = boolean(default=False)
batch_size = integer(default=500)
