[OUTPUT]
output_path = output
img_interval = 1        # save plots of the RBM's state every N epochs
dump_interval = 50      # save the RBM state every N epochs

[INPUT]
ngram_size = 1          # length of an ngram (1 for images, n for any time slices)

[TOPOLOGY]
[[RBM1]]
type_net = RBMPCDGOH
n_hidden = 300          # the number of units in the hidden layer
epochs = 101            # the number of times all data is iterated for training
learning_rate = 0.1     # how fast do the parameters of the model adapt
batch_size = 200        # the number of data instances used for a parameter update
wl1 = 0.          # penalty for the L1 norm of parameters
wl2 = 1e-3        # penalty for the L2 norm of parameters
momentum = 0.5          # how strongly do past updates influence the current update

reduce_lr = True        # if the learning rate should be reduced to zero during training

dropout_h = 0.5         # dropout on the hidden units
dropout_v = 0.1         # dropout on the visible units

# GOH sparsity params
mu = 0.1                # how steep the target sparsity/selectivity curve should be
phi = 0.5               # blending factor of the target sparsity/selectivity

fantasy_particles = 200 # number of fantasy particles in PCD
activation_crop = 0.75  # shuffles weights of units with activation > 0.75
                        # over training examples

[[RBM2]]
type_net = RBMPCDGOH
n_hidden = 250
epochs = 101
learning_rate = 0.05
batch_size = 200
n_cd = 1
wl1 = 0.
wl2 = 1e-3
momentum = 0.5

reduce_lr = True

mu = 0.03
phi = 0.8

fantasy_particles = 50
activation_crop = 0.75

[[NNTop]]
# Only used for fine-tuning (STACK)
# Therefore only a few parameters are important
# The layer itself will not get trained (epochs = 0)
type_net = NNSoftmaxBN
n_hidden = 10
epochs = 0
learning_rate = 0.05

wl1 = 0.
wl2 = 0.001
momentum = 0.5

[STACK]
# parameters have the same meaning as in single layers
img_interval = 50
dump_interval = 50

momentum = 0.85
batch_size = 200

learning_rate = 5e-5
epochs = 501

reduce_lr = False