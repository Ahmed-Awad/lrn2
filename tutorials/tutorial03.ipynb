{
 "metadata": {
  "name": "",
  "signature": "sha256:3523a3733aa6b0132b2318c1d689e89339579c8c4c800ca2fb1e7b248269c11b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Configuration File"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Specifying the NN architecture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The lrn2 framework provides the method <a href=\"http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#lrn2.nn_bricks.make.make_net\">make_net</a>, which creates a list of NN layers, as specified in a configuration file (example: see below). Such a list of layers can then get trained layer-wise (i.e. Restricted Boltzmann Machine training) by calling <a href=\"http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#lrn2.nn_bricks.train.train_layer_wise\">train_layer_wise</a>. A single layer or a stack can be trained with <a href=\"http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#lrn2.nn_bricks.train.train_cached\">train_cached</a> (see next tutorial section below).\n",
      "\n",
      "A parameter to the <a href=\"http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#lrn2.nn_bricks.make.make_net\">make_net</a> method is <code>config</code>, a dictionary created with the method <a href=\"http://lrn2cre8.ofai.at/lrn2/doc/lrn2.util.html?highlight=get_config#lrn2.util.config.get_config\">get_config(files)</a>, where <code>files</code> is the filepath to one or more configuration files.\n",
      "\n",
      "The example below shows a configuration file specifying a two-layered stacked RBM, as well as a Backpropagation section."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the configuration file\n",
      "with open(\"config_model.ini\", 'r') as config_file:\n",
      "    print config_file.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[OUTPUT]\n",
        "output_path = output\n",
        "img_interval = 1        # save plots of the RBM's state every N epochs\n",
        "dump_interval = 50      # save the RBM state every N epochs\n",
        "\n",
        "[INPUT]\n",
        "ngram_size = 1          # length of an ngram (1 for images, n for any time slices)\n",
        "\n",
        "[TOPOLOGY]\n",
        "[[RBM1]]\n",
        "type_net = RBMPCDGOH\n",
        "n_hidden = 300          # the number of units in the hidden layer\n",
        "epochs = 101            # the number of times all data is iterated for training\n",
        "learning_rate = 0.1     # how fast do the parameters of the model adapt\n",
        "batch_size = 200        # the number of data instances used for a parameter update\n",
        "wl1_param = 0.          # penalty for the L1 norm of parameters\n",
        "wl2_param = 1e-3        # penalty for the L2 norm of parameters\n",
        "momentum = 0.5          # how strongly do past updates influence the current update\n",
        "\n",
        "reduce_lr = True        # if the learning rate should be reduced to zero during training\n",
        "\n",
        "dropout_h = 0.5         # dropout on the hidden units\n",
        "dropout_v = 0.1         # dropout on the visible units\n",
        "\n",
        "# GOH sparsity params\n",
        "mu = 0.1                # how steep the target sparsity/selectivity curve should be\n",
        "phi = 0.5               # blending factor of the target sparsity/selectivity\n",
        "\n",
        "fantasy_particles = 200 # number of fantasy particles in PCD\n",
        "activation_crop = 0.75  # shuffles weights of units with activation > 0.75\n",
        "                        # over training examples\n",
        "\n",
        "[[RBM2]]\n",
        "type_net = RBMPCDGOH\n",
        "n_hidden = 250\n",
        "epochs = 101\n",
        "learning_rate = 0.05\n",
        "batch_size = 200\n",
        "n_cd = 1\n",
        "wl1_param = 0.\n",
        "wl2_param = 1e-3\n",
        "momentum = 0.5\n",
        "\n",
        "reduce_lr = True\n",
        "\n",
        "mu = 0.03\n",
        "phi = 0.8\n",
        "\n",
        "fantasy_particles = 50\n",
        "activation_crop = 0.75\n",
        "\n",
        "[[NNTop]]\n",
        "# Only used for fine-tuning (STACK)\n",
        "# Therefore only a few parameters are important\n",
        "# The layer itself will not get trained (epochs = 0)\n",
        "type_net = NNSoftmaxBN\n",
        "n_hidden = 10\n",
        "epochs = 0\n",
        "learning_rate = 0.05\n",
        "\n",
        "wl1_param = 0.\n",
        "wl2_param = 0.001\n",
        "momentum = 0.5\n",
        "\n",
        "[STACK]\n",
        "# parameters have the same meaning as in single layers\n",
        "img_interval = 50\n",
        "dump_interval = 50\n",
        "\n",
        "momentum = 0.85\n",
        "batch_size = 200\n",
        "\n",
        "learning_rate = 5e-5\n",
        "epochs = 501\n",
        "\n",
        "reduce_lr = False\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see above, sections in the configuration file are indicated by squared brackets. There is a TOPOLOGY section, indicating where the NN topology is specified. After this, an arbitrary number of NN layers may be defined. The layer labels (i.e. the section names within the TOPOLOGY section) are arbitrary, but they have to be unique. The architecture is built solely based on the structure of the sections. However, the labels of the sections still matter, as files generated during training are partly named after those labels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Parameter <code>type_net</code> accepts several different NN layer types. The list of possible layer types will be constantly extended (and can easily get extended by custom types). Therefore it is recommended to check the file <code>lrn2.util.config_spec.ini</code>, which reflects all the possible layer types which can be currently specified in the configuration file, or see the [make section](http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#module-lrn2.nn_bricks.make) in the Documentation. Note that all classes starting with 'FFNN..' are stacks and should not be specified via the configuration file."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Specifying Backpropagation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another training method in the framework is <a href=\"http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#lrn2.nn_bricks.train.train_cached\">lrn2.models.nn_bricks.train.train_cached()</a>, which optimizes the parameters of a single layer or an NN stack. This backpropagation procedure can also be specified in a configuration file using the section STACK (see config file above). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "[STACK]\n",
      "\n",
      "img_interval = 20\n",
      "\n",
      "dump_interval = 50\n",
      "\n",
      "learning_rate = 0.0005\n",
      "\n",
      "epochs = 500\n",
      "\n",
      "batch_size = 100\n",
      "\n",
      "reduce_lr = True\n",
      "\n",
      "momentum = 0.6"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an example, we create a list of NN layers, which we pass to the constructor of the NN stack `FFNNCrossEntropy` (all modules in the [make section](http://lrn2cre8.ofai.at/lrn2/doc/lrn2.nn_bricks.html#module-lrn2.nn_bricks.make) starting with FFNN.. are stacks and should be initialized that way. Note also that all stacks derive from `NNStack`). Subsequently, we train the whole stack with backpropagation using our example configuration file. Regularizers specific to layers (e.g. wl1, wl2 weight regularizations) are automatically included in the global cost function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from lrn2.util.config import get_config\n",
      "from lrn2.nn_bricks.make import make_net, FFNNCrossEntropy\n",
      "\n",
      "# import configuration file\n",
      "config = get_config(\"config_model.ini\")\n",
      "\n",
      "# create a list of NN layers\n",
      "rbms = make_net(config, input_shape = 3)\n",
      "\n",
      "# Create a stack from the list of NN layers\n",
      "nn_stack = FFNNCrossEntropy(layers = rbms, name = \"Stack\")\n",
      "\n",
      "print nn_stack"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<lrn2.nn_bricks.make.FFNNCrossEntropy object at 0x7ff44997a9d0>\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from lrn2.nn_bricks.train import train_cached\n",
      "\n",
      "# dummy data for tutorial\n",
      "data_source = np.array([[0.,0.,0.],[0.,0.,0.]])\n",
      "data_target = np.array([[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.], [1.,1.,1.,1.,1.,1.,1.,1.,1.,1.]])\n",
      "#batches = [data_source, data_target]\n",
      "#batches = [np.vstack([batches[i][j] for i in range(len(batches))]) for j in range(len(batches[0]))]\n",
      "\n",
      "# Bind data on variables\n",
      "data = {'input': data_source, 'target': data_target}\n",
      "# train the whole stack with backpropagation\n",
      "nn_stack = train_cached(net = nn_stack, data = data, config = config, run_keyword = \"tutorial\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}